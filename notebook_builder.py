# notebook_builder.py
from __future__ import annotations
import json
from typing import Optional, Dict, Any
import nbformat as nbf
from datetime import datetime

HEADER_MD = """# UNISOLE Auto-AI: ML Notebook (Education Mode)

This notebook was generated by **UNISOLE Auto-AI LAB**.  
It includes:
- Data loading
- Mini-EDA (quick visual checks)
- Optional **Feature Engineering pipeline**
- **AutoML training** (scikit-learn models)
- Metrics & model card
- Example: using the saved model for inference

> Tip: Run cells top-to-bottom. Customize freely. Learn by editing ðŸ§ 
"""

def _mk(code: str) -> Any:
    return nbf.v4.new_code_cell(code.strip("\n"))

def _md(text: str) -> Any:
    return nbf.v4.new_markdown_cell(text.strip("\n"))

def build_notebook_bytes(
    *,
    csv_path: str,
    target: str,
    task: str,
    model_card: Optional[Dict[str, Any]] = None,
    saved_model_path: Optional[str] = None,
    saved_pipeline_path: Optional[str] = None,
    include_mini_eda: bool = True,
    include_llm_intro: Optional[str] = None,
    kernel_name: str = "auto_ml",
) -> bytes:
    nb = nbf.v4.new_notebook()
    nb["metadata"]["kernelspec"] = {
        "display_name": kernel_name,
        "language": "python",
        "name": kernel_name
    }
    cells = []

    # Title + intro
    intro = HEADER_MD
    if include_llm_intro:
        intro += f"\n---\n### Mentor Notes\n{include_llm_intro}\n"
    cells.append(_md(intro))

    # Setup
    cells.append(_md("## 1) Setup & Imports"))
    cells.append(_mk("""
import sys, os, json, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, r2_score, mean_absolute_error, mean_squared_error
import joblib
sns.set(style="whitegrid")
print("Python version:", sys.version)
print("Working directory:", os.getcwd())
"""))

    # Load data
    cells.append(_md("## 2) Load Dataset"))
    cells.append(_mk(f"""
CSV_PATH = r"{csv_path}"
TARGET = "{target}"
df = pd.read_csv(CSV_PATH)
print(df.shape)
df.head()
"""))

    # Mini EDA
    if include_mini_eda:
        cells.append(_md("## 3) Mini-EDA"))
        cells.append(_mk("""
display(df.describe(include='all').transpose().fillna(""))
df.isna().mean().sort_values(ascending=False).head(20)
"""))
        cells.append(_mk("""
# Numeric correlation heatmap (quick)
num = df.select_dtypes(include=[np.number])
if num.shape[1] >= 2:
    import matplotlib.pyplot as plt
    import seaborn as sns
    plt.figure(figsize=(7,5))
    sns.heatmap(num.corr(), cmap="coolwarm", center=0)
    plt.title("Correlation Heatmap")
    plt.show()
"""))
        cells.append(_mk("""
# Target distribution preview
import matplotlib.pyplot as plt
plt.figure(figsize=(6,4))
if pd.api.types.is_numeric_dtype(df[TARGET]):
    df[TARGET].hist(bins=30)
    plt.title(f"Target Distribution: {TARGET}")
else:
    df[TARGET].value_counts().plot(kind="bar")
    plt.title(f"Target Classes: {TARGET}")
plt.show()
"""))

    # Optional feature pipeline
    if saved_pipeline_path:
        cells.append(_md("## 4) Feature Engineering (using saved pipeline)"))
        cells.append(_mk(f"""
PIPELINE_PATH = r"{saved_pipeline_path}"
pipe = joblib.load(PIPELINE_PATH)
print("Loaded feature pipeline:", type(pipe))
# Transform entire dataset (X only); keep target aside
X = df.drop(columns=[TARGET])
y = df[TARGET]
X_proc = pipe.transform(X)
print("Processed shape:", X_proc.shape)
"""))
    else:
        cells.append(_md("## 4) (Optional) Simple split without FE pipeline"))
        cells.append(_mk("""
X = df.drop(columns=[TARGET])
y = df[TARGET]
print("Raw shapes:", X.shape, y.shape)
"""))

    # Train/test split
    cells.append(_md("## 5) Train / Test Split"))
    split_code = """
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X_proc if 'X_proc' in globals() else X, y, test_size=0.2, random_state=42,
    stratify=y if not pd.api.types.is_numeric_dtype(y) else None
)
X_train.shape, X_test.shape
"""
    cells.append(_mk(split_code))

    # Load best model (from platform)
    if saved_model_path:
        cells.append(_md("## 6) Load Best Model (from Auto-AI)"))
        cells.append(_mk(f"""
MODEL_PATH = r"{saved_model_path}"
model = joblib.load(MODEL_PATH)
model
"""))
    else:
        # Fallback: simple baseline
        algo = "LogisticRegression" if task == "classification" else "RandomForestRegressor"
        cells.append(_md("## 6) Train a Simple Baseline Model (fallback)"))
        if task == "classification":
            cells.append(_mk("""
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
"""))
        else:
            cells.append(_mk("""
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=300, random_state=42)
model.fit(X_train, y_train)
"""))

    # Evaluate
    cells.append(_md("## 7) Evaluate Model"))
    if task == "classification":
        eval_code = """
from sklearn.metrics import classification_report
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
"""
    else:
        eval_code = """
y_pred = model.predict(X_test)
print("R2:", r2_score(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("RMSE:", mean_squared_error(y_test, y_pred) ** 0.5)
"""
    cells.append(_mk(eval_code))

    # Model card
    if model_card:
        cells.append(_md("## 8) Model Card (from Auto-AI)"))
        pretty = json.dumps(model_card, indent=2)
        cells.append(_mk(f"model_card = {pretty}\nmodel_card"))

    # Inference example
    cells.append(_md("## 9) How to Use the Saved Model (Inference)"))
    cells.append(_mk("""
# Example: predict on first 3 rows of X_test
sample = X_test[:3]
preds = model.predict(sample)
print("Predictions for first 3 rows:", preds)
"""))

    # Save as .ipynb bytes
    nb["cells"] = cells
    return nbf.writes(nb).encode("utf-8")
